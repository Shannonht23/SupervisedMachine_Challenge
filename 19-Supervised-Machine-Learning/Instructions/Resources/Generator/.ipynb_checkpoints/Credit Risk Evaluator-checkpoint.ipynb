{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data:\n",
    "\n",
    "XTrain = pd.get_dummies(train_df.drop(columns=['target']))\n",
    "YTrain = train_df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prediction: 'make a prediction as to which model you think will perform better and why ' ? \n",
    "#I believe that random forest will perform better than Logistics Regression because it randomly selects subsets of \n",
    "#features and can isolate more important features which helps in increasing the overall accuracy of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "\n",
    "XTest = pd.get_dummies(test_df.drop(columns=['target']))\n",
    "YTest = test_df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing dummy variables to testing set\n",
    "\n",
    "for col in XTrain.columns:\n",
    "        if col not in XTest.columns:\n",
    "            XTest[col]= 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5085070182900894"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the unscaled data and print the model score\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(XTrain, YTrain)\n",
    "modelLR.score(XTest, YTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6373883453849426"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "\n",
    "modelRC = RandomForestClassifier()\n",
    "modelRC.fit(XTrain, YTrain)\n",
    "modelRC.score(XTest, YTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model performed better? How does that compare to your prediction? Write down your results and thoughts.\n",
    "\n",
    "# The Random Forest Classifier seem to have performed better than Logistics Regression on our current data because from what the \n",
    "#results that are shown Logistics Regression mas 0.508 percent verse random forest classifier that score was 0.637.  I\n",
    "#believe my results of RFC falls in line with my prediction.  \n",
    "\n",
    "2.\n",
    "#Before re-fitting the LogisticRegression and RandomForestClassifier models on the scaled data, \n",
    "#make another prediction about how you think scaling will affect the accuracy of the models. \n",
    "#Write your predictions down and provide justification.\n",
    "\n",
    "#I believe that with the data being scaled that the data will be more precise and fall in line with RandomForestClassifier\n",
    "#still being more accurate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaledata = StandardScaler()\n",
    "scaledata.fit(XTrain)\n",
    "XTRNScaled = scaledata.transform(XTrain)\n",
    "XTEScaled = scaledata.transform(XTest)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598894087622289"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score\n",
    "\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(XTRNScaled, YTrain)\n",
    "modelLR.score(XTEScaled, YTest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.635899617184177"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "\n",
    "\n",
    "modelRC = RandomForestClassifier()\n",
    "modelRC.fit(XTRNScaled, YTrain)\n",
    "modelRC.score(XTEScaled, YTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does the model scores compare to each other, and to the previous results on unscaled data? \n",
    "#How does this compare to your prediction? Write down your results and thoughts.\n",
    "\n",
    "#After the data was scalled the data became more precise and fine tuned. However from what I can see Logistic Regression seem to have\n",
    "#made the greatest improvement from 51 percent to 76 percent. While the RFC  went done by a points, but generally stayed the \n",
    "#the same. 0.637 down to 0.635. As we can see the with Scaled date Logistics Regression worked out better in accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
